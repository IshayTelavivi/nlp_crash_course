{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 05E - Two convolutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features) # instead on nb_words="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can use another argument here, which is 'skip_top=', and that to ignore X top most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define maximum length of 400. It means that posts shoeter than 400 will be filled with 0s, and longer posts are cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two arrays were padded, and the third one was cut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now defining the model**\n",
    "\n",
    "As opposed to the original file, I an creating the model in a function.\n",
    "\n",
    "Also gathering all the imports from the original version (05) in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras models can be used in scikit-learn by wrapping them with the **KerasClassifier** or **KerasRegressor** class.\n",
    "\n",
    "To use these wrappers you must define a function that creates and returns your Keras sequential model, then pass this function to the build_fn argument when constructing the KerasClassifier class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    inputs_shape1 = Input(shape=(400,))\n",
    "    embed1 = Embedding(input_dim=5000, output_dim=50)(inputs_shape1)\n",
    "    spat_drop1 = SpatialDropout1D(0.4)(embed1)\n",
    "    # First conv\n",
    "    conv1 = Conv1D(filters=250, kernel_size=3, padding='valid', activation='relu', strides=1)(spat_drop1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "    # Second conv\n",
    "    conv2 = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=2)(pool1)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "    # Flatten\n",
    "    flat = Flatten()(pool2)\n",
    "    # Now the fully connected layers\n",
    "    dense1 = Dense(250, activation='relu')(flat)\n",
    "    drop1 = Dropout(0.3)(dense1)\n",
    "    dense2 = Dense(20, activation='relu')(drop1)\n",
    "    drop2 = Dropout(0.3)(dense2)\n",
    "    output = Dense(1, activation='sigmoid')(drop2)\n",
    "    model = Model(inputs=inputs_shape1, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the KerasClassifier I include the original fit agruments (batch_size, epochs, verbose), unless I want to grid search them, then I include them in the param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=1, epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 398, 250)          37750     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 199, 250)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 98, 100)           100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 49, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               1225250   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                5020      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,618,141\n",
      "Trainable params: 1,618,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 16s 650us/step - loss: 0.6935 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 15s 583us/step - loss: 0.6097 - acc: 0.6032 - val_loss: 0.3155 - val_acc: 0.8651\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 15s 583us/step - loss: 0.2933 - acc: 0.8867 - val_loss: 0.3232 - val_acc: 0.8617\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 15s 584us/step - loss: 0.2268 - acc: 0.9152 - val_loss: 0.2961 - val_acc: 0.8794\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 15s 583us/step - loss: 0.1894 - acc: 0.9309 - val_loss: 0.3098 - val_acc: 0.8800\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 32\n",
    "h = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "              validation_data=(X_test, y_test), verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
