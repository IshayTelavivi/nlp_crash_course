{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! Important!!!\n",
    "We do not automatically remove numbers and characters or transforming to lowercase. This depends on the our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"Mr. Sweet Potatoes, and Other Stories by Anonymous.txt\", \"r\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook, Mr. Sweet Potatoes an'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Replace nums to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook, Mr. Sweet Potatoes and Other Stories, by Anonymous\n",
      "\n",
      "This eBook is for t\n"
     ]
    }
   ],
   "source": [
    "digits = re.compile(r\"\\d[\\d\\.\\$]*\")\n",
    "text = digits.sub(\"<NUM>\", raw_text)\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Remove characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook Mr Sweet Potatoes and Other Stories by Anonymous\n",
      "\n",
      "This eBook is for the \n"
     ]
    }
   ],
   "source": [
    "not_allowed = re.compile(r\"[^\\s\\w<>_]\")\n",
    "text = not_allowed.sub(\"\", text)\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to use *string.punctuation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trantab = str.maketrans(\"\", \"\", string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook Mr Sweet Potatoes and Other Stories by Anonymous\n",
      "\n",
      "This eBook is for the \n"
     ]
    }
   ],
   "source": [
    "# punc_text = text.translate(trantab)\n",
    "# print(punc_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the project gutenberg ebook mr sweet potatoes and other stories by anonymous\n",
      "\n",
      "this ebook is for the \n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'mr', 'sweet', 'potatoes', 'and', 'other', 'stories']\n"
     ]
    }
   ],
   "source": [
    "word_list = text.split()\n",
    "print(word_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Stem and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'mr', 'sweet', 'potato', 'stori', 'anonym', 'ebook', 'use']\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "stem_word_list = [stemmer.stem(word) for word in word_list if word not in stop]\n",
    "print(stem_word_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**with nltk word_tokenize**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This step automatically does the 4 steps above (numbers, characters, split to words and lowercase) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'mr', 'sweet', 'potatoes', 'and', 'other', 'stories']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = word_tokenize(text)\n",
    "print(tokenized_text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Stem and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'mr', 'sweet', 'potato', 'stori', 'anonym', 'ebook', 'use']\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "stem_word_list = [stemmer.stem(word) for word in tokenized_text if word not in stop]\n",
    "print(stem_word_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying out sent_tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook, Mr. Sweet Potatoes and Other Stories, by Anonymous\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(raw_text)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
